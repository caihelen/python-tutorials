{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Tutorial 3!\n",
    "You're all doing great so far! After this tutorial, you will be able to write your own motif searching algorithm, and use the results for your project.  \n",
    "In this tutorial, we will be learning about file input and output (file I/O) and regular expressions (regex). You're in the home stretch!  \n",
    "![](https://media.giphy.com/media/xUOxfh6ZM75efM3Bqo/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: File Input\n",
    "Using Python to read and write to files is very helpful when working with larger datasets. The kinds of files we'll be opening and closing are often text files, like you'd write in Notepad or any other text editor.  \n",
    "\n",
    "In the future, you will most likely encounter these two types of text files: CSVs and TSVs. __CSV__ stands for \"comma-separated variables,\" and __TSV__ stands for \"tab-separated variables.\" As you might guess, these two file types use commas and tabs, respectively, to separate the contents of their files. These are often representations of __data tables__ and __spreadsheets.__ Putting data inside a CSV or TSV file can be useful for several reasons:\n",
    "* You can access the data and use it even after you close Python.\n",
    "* You can \"clean up\" valuable memory that Python needs in order to work efficiently.\n",
    "* You can back up data that you worked so hard to clean and organize.\n",
    "* You can send it to other researchers and collaborators to use.\n",
    "\n",
    "Let's practice opening and closing some files. We've uploaded a CSV to Canvas called \"climatetweets.csv\". This is from a project called \"Climate Related Tweets Before, During, and After Hurricane Sandy 10-5 through 11-15 2012\" conducted by the University of Central Florida. You can view the data source [here](https://www.openicpsr.org/openicpsr/project/100202/version/V3/view).  \n",
    "\n",
    "There are many ways to open a data file and read in its contents. Fortunately, there is a convenient library we can use called __csv__. Unsurprisingly, it contains functions and tools we can use to work with CSV files. In the following cell we will import the __csv__ library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We now have the functions to start working with CSVs. But, we need to read in each row of the data file in order to start working with it. The following code will create an empty list called `data`, which we will use to hold our data. Then, it will read in each row of the CSV file one at a time and add (or, `append`) it to `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scaih\\Desktop\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-a27f4ccdb458>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"climatetweets.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"latin-1\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcsvDataFile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mcsvReader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsvDataFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcsvReader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "with open(\"climatetweets.csv\", \"r\", encoding=\"latin-1\") as csvDataFile:\n",
    "    csvReader = csv.reader(csvDataFile)\n",
    "    for row in csvReader:\n",
    "        data.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the line with the words `with open( ) as`. This may look strange to you, but it is convention for __opening__ and __closing__ data files in Python. It's an example of what we call __\"syntactic sugar\"__: shorthand code that makes it easier for humans to understand, but doesn't change what the computer actually does. In this case, the word `with` will make sure that our file properly opens and closes, no matter what the code chunk that follows does or fails to do. A very good thing for file security!  \n",
    "\n",
    "Notice also that the function `open` takes in an argument that says `\"r\"`. Can you guess what this `\"r\"` stands for? Write your answer below. (Hint: it may be useful to use the `help` function to look at the documentation for `open`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first ~6 entries in our \"__data__\" list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, that was a jumble of stuff. It seems like the first list (or \"row\" in our data table) is a series of dates, while the second list is a bunch of tweets itself. Let's focus in on some of the tweets in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all a mess!  \n",
    "\n",
    "When you're dealing with raw data, more times than not the data is very messy. We can do some tidying up in order to get more sense out of it. When we start to clean up text, __regular expressions__ will be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Regex\n",
    "Regex, or regular expression, is a very powerful concept. It is essentially posing a series of search queries to match the information you are trying to find. \n",
    "A \"regex\" is a series of characters and symbols that represent a search pattern you are trying to find in a much longer sequence. Here's a regex \"cheat sheet\":\n",
    "![](http://cfile27.uf.tistory.com/image/1642BD334E6A9A0E02B013)  \n",
    "\n",
    "Unsurprisingly, there is a very useful Python library with functions for working with regular expressions. It is called `re`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to import the library!\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next few cells, we are going to play around with regular expressions and clean up some tweets. If you're interested in learning more about regular expressions, [this website](https://www.regular-expressions.info/reference.html) is a good place to start.  \n",
    "\n",
    "Let's first look at some example tweets, just to get an idea of the sorts of cleaning we are going to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_tweet_1 = data[1][2]\n",
    "ex_tweet_2 = data[1][6]\n",
    "ex_tweet_3 = data[1][8]\n",
    "\n",
    "for tweet in [ex_tweet_1, ex_tweet_2, ex_tweet_3]:\n",
    "    print(tweet, '\\n') #Separate each tweet by a newline ('\\n') to make it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very interesting. It seems like each tweet has an ellipsis (\"...\") at the front and beginning of each tweet. It also seems like there are some (broken) links embedded within the tweets.  \n",
    "\n",
    "If we want to clean these tweets to make them more *readable* (not necessarily more *informative*), here are some possible to-do items for us:\n",
    "* Strip off the leading and trailing ellipses\n",
    "* Remove any retweet flags  \n",
    "* Remove any links\n",
    "\n",
    "\n",
    "Let's dive right into using regex. We'll use our example tweets to test everything out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ex_tweet_1, '\\n')\n",
    "\n",
    "# Use the re.sub() function to substitute characters\n",
    "ex_tweet_1_clean = re.sub('\\...', '', ex_tweet_1)\n",
    "print(ex_tweet_1_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we used the `re.sub` function to find characters and replace them. According to the [documentation for this function](https://docs.python.org/2/library/re.html#module-contents), `re.sub` needs at least 3 arguments. \n",
    "* The first argument is the __regex pattern__ we are searching for. In this example, we needed to search for three dots \"...\" together. However, a *single dot* is a special character! In order to get the *literal* string of three dots, we needed to use a backslash to __escape__ the special-character meaning of a single dot. (Try removing the backslash (\"`\\`\") and see what happens!)\n",
    "* The second argument is the text that will be __substituted__ at every location the __regex pattern__ is found. In this case, we wanted to replace the three dots with \"nothing\", so I used empty quotes. (Try putting something inside of the quotes and see what happens!)\n",
    "* The third argument is the __text__ that we are searching through. Since I just wanted to try out this regex on our first example tweet, I let this argument be `ex_tweet_1`. (Try it on `ex_tweet_2`, `ex_tweet_3`, or another string that you come up with and see what happens!)  \n",
    "\n",
    "Let's try the exact same code on the second example tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ex_tweet_2, '\\n')\n",
    "\n",
    "# Remove the ellipses as we did with the first tweet\n",
    "ex_tweet_2_clean = re.sub('\\...', '', ex_tweet_2)\n",
    "print(ex_tweet_2_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! What happened?  \n",
    "It looks like we accidentally removed too many dots from the end of this tweet. The word \"enough\" was cut off, and the tweet used dots to signify that it was a shortened word. We'll need a more specific regex that only takes off *three* dots at a time. Luckily, there is a regex expression that signifies the quantity \"exactly three of.\" You just put the number `3` inside curly brackets (`{ }`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ex_tweet_2, '\\n')\n",
    "\n",
    "# It's good practice to work with a copy of the original text, in case something goes wrong\n",
    "ex_tweet_2_clean = ex_tweet_2\n",
    "# Remove exactly three ellipses from beginning of string\n",
    "ex_tweet_2_clean = re.sub('^\\.{3}', '', ex_tweet_2_clean)\n",
    "# Remove exactly three ellipses from end of string\n",
    "ex_tweet_2_clean = re.sub('\\.{3}$', '', ex_tweet_2_clean)\n",
    "print(ex_tweet_2_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our fix worked! We also used the regex characters `^` (indicating the pattern must be at the beginning of the string) and `$` (indicating the pattern must be at the end of the string).  \n",
    "Now that we've fixed this issue, let's go on to the second thing to do: removing retweet flags. It seems that retweet flags have a pattern to them. They always begin with the capital letters \"RT\", followed by an `@` symbol indicating the user the tweet is retweeted from. We can design a regex to match this pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_tweet_2_clean = re.sub('RT\\s@[a-zA-Z:]+', '', ex_tweet_2_clean)\n",
    "print(ex_tweet_2_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Let's unpack the regex we used. The regular expression will search for a pattern that has the following things, in *this order*:\n",
    "* `RT` tells the pattern that we want the *exact* characters 'RT' at the *beginning* of the search pattern. \n",
    "* `\\s` means a single space.\n",
    "* `@` will tell the pattern to look for '@'. '@' is not a special character in regex, so we do not need to escape it with a backslash.\n",
    "* `[a-zA-Z:]` will search for any lowercase letter, uppercase letters, or semicolon (\":\"). The *square brackets* will tell Python that the character in this position can be any of these characters. As you might expect, `a-z` is shorthand for any lowercase letter, and `A-Z` is shorthand for any uppercase letter.\n",
    "* `+` tells the pattern to search for the previous character \"multiple times.\" So, it will search for multiple instances of lowercase letters, uppercase letters, and semicolons. (The subtlety here is that we did not specify a space \" \" in the previous search character. So, the regex will stop trying to match the pattern once it sees a space in the original text.)  \n",
    "\n",
    "We can go on to apply these `re.sub` patterns to each of the tweets in our dataset. But, remember that we don't necessarily want to copy and paste code over and over again in order to clean each of the tweets! It's best to put all of our data-cleaning work inside of our own data-cleaning *function*. Then, we can use a *loop* to apply our function to every tweet in our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code! Each line of this function should look familiar.\n",
    "def cleanTweets(tweet):\n",
    "    cleaned_tweet = tweet\n",
    "    cleaned_tweet = re.sub('^\\.{3}', '', cleaned_tweet)\n",
    "    cleaned_tweet = re.sub('\\.{3}$', '', cleaned_tweet)\n",
    "    cleaned_tweet = re.sub('RT\\s@[a-zA-Z:]+', '', cleaned_tweet)\n",
    "    return(cleaned_tweet)\n",
    "\n",
    "# Check to see if our function works like we want it to\n",
    "print(ex_tweet_3, '\\n')\n",
    "print(cleanTweets(ex_tweet_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Let's apply it to all of the tweets in the first \"row\" of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data[1])):\n",
    "    data[1][i] = cleanTweets(data[1][i])\n",
    "print(data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks a little bit more readable than what it was before!  \n",
    "Keep in mind that this is just barely scratching the surface of what is possible with regex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn!\n",
    "The last thing that we might want to do is remove links. From our experience, it seems that each link will start with an \"http\". Your task is to modify the function `cleanTweets` to remove links. To do this, you will need to:\n",
    "* Add some code to the `cleanTweets` in order to remove links, by:\n",
    "* Designing a simple regex to search for the characters \"http\", followed by any sequence of multiple letters, characters, and numbers. The cheat sheet included above may come in handy. \n",
    "* Apply your modified function to any row of tweet data to see how it works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweets(tweet):\n",
    "    cleaned_tweet = tweet\n",
    "    cleaned_tweet = re.sub('^\\.{3}', '', cleaned_tweet)\n",
    "    cleaned_tweet = re.sub('\\.{3}$', '', cleaned_tweet)\n",
    "    cleaned_tweet = re.sub('RT\\s@[a-zA-Z:]+', '', cleaned_tweet)\n",
    "    # Your Regex After This Line\n",
    "    \n",
    "    return(cleaned_tweet)\n",
    "\n",
    "# Your Code To Apply Function After This Line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: File Output\n",
    "Since we've been working with a list of tweets, let's write out our cleaned tweets to another CSV file. We will do this with functions from--you guessed it--the __csv__ library. The following code will:\n",
    "* `open` a new, blank CSV file to write to. We called it  \"cleaned_climatetweets.csv\"\n",
    "* create a function called `\"spamwriter\"` that works to `write` data to the CSV file\n",
    "* clean each row of data using the cleanTweets function\n",
    "* write each row of cleaned data to the \"cleaned_climatetweets.csv\" file, using our preivously defined `\"spamwriter\"` function\n",
    "* `close` the CSV file that we were wrote our data to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cleaned_climatetweets.csv\", \"w\") as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',', quotechar='\"')\n",
    "    for row in data:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = cleanTweets(row[i])\n",
    "        spamwriter.writerow([row])\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we used the words `with open( ) as` again to write into a new CSV file. This time, we also used a parameter `\"w\"`. What does the `\"w\"` mean here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn!\n",
    "In the folder on your computer where this Jupyter Notebook is, you will now have a .csv file with all of our stored data. Your task is to:\n",
    "1. Try opening it. \n",
    "2. Read in your data, row by row, and add (or, `append`) each row to the variable `cleaned_data`.\n",
    "3. Show us the third row of your data. (Remember that counting in Python starts at 0!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = []\n",
    "# Your Code Here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
